{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "from tqdm.notebook import tqdm\n",
    "sns.set()\n",
    "rcParams['figure.figsize'] = (20,10)\n",
    "pd.options.display.max_columns = None\n",
    "warnings.filterwarnings('ignore')\n",
    "from pprint import pprint\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from googletrans import Translator\n",
    "import time\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, pipeline\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bigscience/bloom-3b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "def translate_df(df, batch_size=1, translator=translator):\n",
    "    def translate_text(text, sleep_time=2):\n",
    "        if text in [''] or text is None:\n",
    "            return ''\n",
    "        while True:\n",
    "            try:\n",
    "                return translator.translate(text, src=\"en\", dest=\"vi\").text\n",
    "            except:\n",
    "                print(\"Requests error\")\n",
    "                time.sleep(sleep_time)\n",
    "    \n",
    "    def clean_text(text):\n",
    "        return text\n",
    "    \n",
    "    data = defaultdict(list)\n",
    "    for i in tqdm(range(0, len(df), batch_size)):\n",
    "        batch_df = df.iloc[i : i + batch_size]\n",
    "        temp_data = defaultdict(list)\n",
    "        for col in df.columns:\n",
    "            batch_df[col] = batch_df[col].apply(lambda x: clean_text(x))\n",
    "            col_vals = ' ## '.join(batch_df[col].values)\n",
    "            temp_data[col] = [x.strip() for x in translate_text(col_vals).split('##')]\n",
    "\n",
    "        can_insert_batch = True\n",
    "        for col in df.columns:\n",
    "            if len(temp_data[col]) != len(batch_df):\n",
    "                can_insert_batch = False\n",
    "                break\n",
    "\n",
    "        if can_insert_batch:\n",
    "            for col in df.columns:\n",
    "                data[col].extend(batch_df[col].values)\n",
    "                data[f'{col}_translated'].extend(temp_data[col])\n",
    "            continue\n",
    "        \n",
    "        # else loop through each row\n",
    "        for _, row in batch_df.iterrows():\n",
    "            for col in df.columns:\n",
    "                text = clean_text(row[col])\n",
    "                data[col].append(text)\n",
    "                data[f'{col}_translated'].append(translate_text(text))\n",
    "                \n",
    "    df = pd.DataFrame(data)\n",
    "    return df.drop(columns=[col for col in df.columns if '_translated' not in col]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"\"\"Cuộc trò chuyện giữa con người và trợ lý AI.\n",
    "[|Con người|] {prompt}\n",
    "[|AI|] {response}\n",
    "[|Con người|]\"\"\"\n",
    "\n",
    "def get_prompt(row):\n",
    "    return a.format(prompt=row['prompt'], response=row['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_json('../../data/translated/vi_alpaca_reduced.jsonl', lines=True)\n",
    "temp2 = temp.apply(lambda x: get_prompt(x), axis=1)\n",
    "temp2 = pd.DataFrame(temp2)\n",
    "temp2.columns = ['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3 = pd.read_json('../../data/translated/quora_chat_data_translated.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3.to_json('../../data/translated/quora_chat_data_translated.json',lines=True,orient='records')\n",
    "temp2.to_json('../../data/translated/alpaca_chat_cleaned_translated.json',lines=True,orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean ShareGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_gpt = pd.read_json('../../data/original/ShareGPT_V3_unfiltered_cleaned_split.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_programming_keywords = [\n",
    "    \"python\",\n",
    "    \"java\",\n",
    "    \"javascript\",\n",
    "    \"c#\",\n",
    "    \"c++\",\n",
    "    \"typescript\",\n",
    "    \"ruby\",\n",
    "    \"swift\",\n",
    "    \"kotlin\",\n",
    "    \"php\",\n",
    "    \"rust\",\n",
    "    \"scala\",\n",
    "    \"dart\",\n",
    "    \"matlab\",\n",
    "    \"objective-c\",\n",
    "    \"perl\",\n",
    "    \"lua\",\n",
    "    \"assert\",\n",
    "    \"async\",\n",
    "    \"await\",\n",
    "    \"def\",\n",
    "    \"elif\",\n",
    "    \"lambda\",\n",
    "    \"nonlocal\",\n",
    "    \"function\",\n",
    "    \"var\",\n",
    "    \"cuda\",\n",
    "    \"torch\",\n",
    "    \"code\",\n",
    "    \"sudo\",\n",
    "    \"bash\"\n",
    "]\n",
    "popular_programming_languages = [\n",
    "    \"python\",\n",
    "    \"java\",\n",
    "    \"javascript\",\n",
    "    \"typescript\",\n",
    "    \"kotlin\",\n",
    "    \"objective-c\",\n",
    "]\n",
    "def check_text_not_contain_code(text):\n",
    "    temp2 = text.lower()\n",
    "    temp = set(temp2.split())\n",
    "    for language in popular_programming_keywords:\n",
    "        if language in temp:\n",
    "            return False\n",
    "    for language in popular_programming_languages:\n",
    "        if language in temp2:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "temp = share_gpt['conversations'].apply(lambda x: str(x)).apply(check_text_not_contain_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_gpt_no_code = share_gpt[temp][['conversations']]\n",
    "share_gpt_no_code = share_gpt_no_code[share_gpt_no_code['conversations'].apply(lambda x: len(x) >= 4)]\n",
    "share_gpt_no_code.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "\n",
    "\n",
    "def create_conversation(turns):\n",
    "    res = \"The conversation between human and AI assistant.\\n\"\n",
    "    for turn in turns:\n",
    "        if turn['from'] == 'human':\n",
    "            res += \"[|Human|] \" + turn['value'] + \"\\n\"\n",
    "        elif turn['from'] == 'gpt':\n",
    "            res += \"[|AI|] \" + turn['value'] + \"\\n\"\n",
    "        else:\n",
    "            return nan\n",
    "    return res\n",
    "\n",
    "share_gpt_no_code_conversations = share_gpt_no_code.apply(lambda x: create_conversation(x['conversations']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(share_gpt_no_code_conversations.sample(1).iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_gpt_no_code_conversations.to_json('../../data/original/share_gpt_no_code_conversations.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/translated/all_faqs.json'\n",
    "faqs = pd.read_json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{instruction}\\n{input}\\nCâu trả lời: {output}\"\"\"\n",
    "print(template.format(**faqs.sample(1).iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = faqs['input'].apply(lambda x: not x.endswith('Điều luật liên quan: '))\n",
    "faqs = faqs[temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faqs['output_len'] = faqs['output'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faqs['output_len'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faqs_standard = faqs[faqs['output_len'] < 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{instruction}\\n{input}\\nCâu trả lời: {output}\"\"\"\n",
    "print(template.format(**faqs_standard.sample(1).iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{instruction}\\n{input}\\nCâu trả lời: {output}\"\"\"\n",
    "print(template.format(**faqs.sample(1).iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faqs_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = Dataset.from_json('/Users/phamhoang1408/Desktop/Phase 2 Viettel/main_repo/data/training/alpaca_chat_cleaned_51k_translated.json')\n",
    "temp.rename_column(temp.column_names[0], 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_json('../../data/original/gpt4-instruct-similarity-0.8-dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds.shuffle()[0]['response'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "from glob import glob\n",
    "\n",
    "def load_dataset(folder_path, dataset_size=None):\n",
    "    data = []\n",
    "    file_paths = glob(folder_path + \"/*.jsonl\")\n",
    "    for path in file_paths:\n",
    "        ds = Dataset.from_json(path)\n",
    "        if len(ds.column_names) != 1:\n",
    "            raise ValueError(\"Dataset must have only one text column\")\n",
    "        ds = ds.rename_column(ds.column_names[0], \"text\")\n",
    "        data.append(ds)\n",
    "    if dataset_size is None:\n",
    "        return concatenate_datasets(data, axis=0).shuffle()\n",
    "    return concatenate_datasets(data, axis=0).shuffle().select(range(dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('../../data/training_31_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.shuffle()[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "def load_dataset(folder_path, dataset_size=None):\n",
    "    data = []\n",
    "    for path in glob(folder_path + \"/*.jsonl\"):\n",
    "        ds = Dataset.from_json(path)\n",
    "        if len(ds.column_names) != 1:\n",
    "            raise ValueError(\"Dataset must have only one text column\")\n",
    "        ds = ds.rename_column(ds.column_names[0], \"text\")\n",
    "        data.append(ds)\n",
    "    if dataset_size is None:\n",
    "        final_ds = concatenate_datasets(data, axis=0).shuffle(seed=42)\n",
    "    final_ds = (\n",
    "        concatenate_datasets(data, axis=0).shuffle(seed=42).select(range(dataset_size))\n",
    "    )\n",
    "    final_ds = final_ds.filter(lambda x: x[\"text\"] != \"\" or x[\"text\"] is not None)\n",
    "    return final_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('../../data/training_31_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(x):\n",
    "    if x == '' or x is None:\n",
    "        return True\n",
    "    return False\n",
    "temp = df['text'].apply(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bigscience/bloomz-3b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tree = []\n",
    "with open('../../data/original/oasst_ready_trees.json') as f:\n",
    "    for line in f:\n",
    "        temp = json.loads(line)\n",
    "        if temp['prompt']['lang'] in ['en']:\n",
    "            en_tree.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_tree = []\n",
    "with open('../../data/original/oasst_ready_trees.json') as f:\n",
    "    for line in f:\n",
    "        temp = json.loads(line)\n",
    "        if temp['prompt']['lang'] in ['vi']:\n",
    "            vi_tree.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(root):\n",
    "    if len(root['replies']) == 0:\n",
    "        return [[(root['role'], root['text'])]]\n",
    "    data = []\n",
    "    for reply in root['replies']:\n",
    "        data.extend(traverse(reply))\n",
    "    data = [[(root['role'], root['text'])] + d for d in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_conversations = []\n",
    "for tree in en_tree:\n",
    "    en_conversations.extend(traverse(tree['prompt']))\n",
    "\n",
    "vi_conversations = []\n",
    "for tree in vi_tree:\n",
    "    vi_conversations.extend(traverse(tree['prompt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21164"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vi_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = \"Cuộc trò chuyện giữa con người và trợ lý AI.\\n{conversation}\"\n",
    "def format(conversations):\n",
    "    data = []\n",
    "    for conversation in conversations:\n",
    "        temp = \"\"\n",
    "        for turn in conversation:\n",
    "            if turn[0] == 'prompter':\n",
    "                temp += f\"[|Con người|] {turn[1]}\\n\"\n",
    "            else:\n",
    "                temp += f\"[|AI|] {turn[1]}\\n\"\n",
    "        data.append(templates.format(conversation=temp))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data = format(en_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_data = format(vi_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conversation between human and AI assistant.\n",
      "[|Human|] Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?\n",
      "[|AI|] Can you write a formal letter to introduce Jeff Bezos to a customer?\n",
      "[|Human|] I asked about contrastive learning in machine learning which has nothing to do with Jeff Bezos. Contrastive learning is used to increase the performance of vision-based tasks using contrast. I want you to explain this technique to me in a way that anyone without machine learning knowledge would understand.\n",
      "[|AI|] In simple terms, contrastive learning focuses on teaching an AI the points of similarity between different images (or other media) to indirectly aid it in spotting points of divergence when present. To anthropomorphize the process, a human engaged in contrastive learning and eating hundreds of apples in a week would be better equipped to recognize an orange when presented with one.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(en_data[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df = pd.DataFrame(en_data, columns=['text'])\n",
    "en_df.to_json('../../data/original/en_oasst.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
