{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "sns.set()\n",
    "rcParams['figure.figsize'] = (20,10)\n",
    "pd.options.display.max_columns = None\n",
    "warnings.filterwarnings('ignore')\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, pipeline\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN_DAILY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Dataset.from_json('../../data/translated/cnn_dailymail_30k_samples_len_100_1200_words_translated.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_template=\"Cuộc trò chuyện giữa con người và trợ lý AI.\\n[|Con người|] Tóm tắt ngắn gọn đoạn văn bản sau đây:\\n{article_translated}\\n[|AI|] \"\n",
    "def mapper(x):\n",
    "    return {\n",
    "        'input': input_template.format(article_translated=x['article_translated']),\n",
    "        'output': 'Đoạn văn bản đã được tóm tắt:\\n' + x['highlights_translated'] + '\\n[|Con người|]'\n",
    "    }\n",
    "\n",
    "temp_cnn = cnn.shuffle().select(range(15000)).map(mapper,batched=False, remove_columns=['article_translated','highlights_translated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_cnn[1]['input'] + temp_cnn[1]['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = temp_cnn.shuffle().select(range(1000)).map(lambda x: {'len':len(tokenizer.tokenize(x['input'] + x['output']))})\n",
    "length.to_pandas().hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cnn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cnn.to_json('../../data/training_3_8/cnn_15k_samples.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIALOG SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog = Dataset.from_json('../../data/translated/dialogsum_10k_samples_len_50_words_translated.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_template=\"Tóm tắt ngắn gọn đoạn hội thoại sau đây:\\n{dialogue_translated}\\nĐoạn hội thoại đã được tóm tắt:\\n\"\n",
    "def mapper(x):\n",
    "    return {\n",
    "        'input': input_template.format(dialogue_translated=x['dialogue_translated']),\n",
    "        'output': x['summary_translated']\n",
    "    }\n",
    "\n",
    "temp_dialog = dialog.shuffle().select(range(10000)).map(mapper,batched=False, remove_columns=['dialogue_translated','summary_translated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_dialog[1]['input'] + temp_dialog[1]['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dialog[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dialog.to_json('../../data/training_3_8/dialogsum_10k_samples.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAQS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faqs = Dataset.from_json('../../data/training_31_7/faq_chat_10k.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = faqs.map(lambda x: {'length':len(tokenizer.tokenize(x['input']))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.length['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(faqs.shuffle()[0]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Cuộc trò chuyện giữa con người và trợ lý AI.\\n'\n",
    "text2 = ' Nếu không đủ thông tin để trả lời thì trả lời: Tôi không biết.'\n",
    "text3 = 'Điều luật liên quan: '\n",
    "text4 = 'Chỉ được trả lời dựa trên điều luật được cung cấp'\n",
    "def mapper(x):\n",
    "    return {\n",
    "        'input': text,\n",
    "        'output': x['input'].removeprefix(text).replace(text2,'').replace(text3, 'Điều luật liên quan:\\n').replace(text4, 'Chỉ được trả lời dựa trên thông tin nằm trong điều luật được cung cấp') + '\\n[|Con người|]',\n",
    "    }\n",
    "\n",
    "temp_faqs = faqs.map(mapper, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp_faqs.shuffle()[0]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp['input'] + temp['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_faqs.to_json('../../data/training_3_8/faqs.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHARE_GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "share = pd.read_json('/Users/phamhoang1408/Desktop/Phase 2 Viettel/main_repo/data/original/share_gpt_no_code_conversations_40k.json', orient='records', lines=True)\n",
    "share.columns = ['input']\n",
    "share.dropna(inplace=True)\n",
    "share.reset_index(drop=True, inplace=True)\n",
    "share = Dataset.from_pandas(share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share = share.filter(lambda x: len(tokenizer.tokenize(x[\"input\"])) < 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share.shuffle().select(range(1000)).map(lambda x: {'len':len(tokenizer.tokenize(x['input']))}).to_pandas().hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b234990cca94874b14a7d7ccfd1d5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38419 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# text = 'Cuộc trò chuyện giữa con người và trợ lý AI.\\n'\n",
    "text = 'The conversation between human and AI assistant.\\n'\n",
    "def mapper(x):\n",
    "    return {\n",
    "        'input': text,\n",
    "        'output': x['input'].removeprefix(text) + '\\n[|Con người|]'\n",
    "    }\n",
    "share = share.map(mapper, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output'],\n",
       "    num_rows: 38419\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'The conversation between human and AI assistant.\\n',\n",
       " 'output': '[|Human|] Summarize the main ideas of Jeff Walker\\'s Product Launch Formula into bullet points as it pertains to a growth marketing agency implementing these strategies and tactics for their clients...\\n[|AI|] Here are the main ideas of Jeff Walker\\'s Product Launch Formula that can be applied by a growth marketing agency for their clients:\\n\\n1. Identify the target audience and their needs: Understand the ideal customer for the product or service,and create a messaging that resonates with them.\\n2. Pre-launch: Build anticipation and excitement for the launch by creating buzz,gathering testimonials and case studies,and using social media to create awareness.\\n3. Launch: Use a well-crafted launch sequence to maximize sales and conversions. This can include offering bonuses,creating scarcity,and using a deadline to create urgency.\\n4. Post-launch: Follow up with customers,gather feedback,and continue to provide value to keep them engaged and loyal.\\n5. Create a product suite: Once the initial product is successful,expand the offering by creating additional products or services that address other needs of the same audience.\\n6. Continual optimization: Continually monitor and optimize the launch process and product suite to improve results.\\n7. Build a community: Use the launch process to build a community of customers who are passionate about the product and can help spread the word.\\n8. Use automation: Use technology and automation to streamline the launch process and improve efficiency.\\n[|Human|] Summarize the main ideas of Brendon Burchard\\'s Experts Academy into bullet points as it pertains to a growth marketing agency implementing these strategies and tactics for their clients...\\n[|AI|] Here are the main ideas of Brendon Burchard\\'s Experts Academy that can be applied by a growth marketing agency for their clients:\\n\\n1. Identify your expertise: Define your area of expertise and create a unique value proposition that differentiates you from others.\\n2. Create a content strategy: Develop a content strategy that showcases your expertise and provides value to your audience. This can include blog posts,videos,podcasts,and social media content.\\n3. Build your audience: Use social media,advertising,and other channels to build your audience and increase your visibility.\\n4. Develop your products: Create products and services that align with your expertise and meet the needs of your audience. These can include digital products,courses,coaching,and consulting services.\\n5. Create a brand: Develop a strong brand that reflects your expertise and resonates with your audience.\\n6. Build authority: Establish yourself as an authority in your industry by speaking at events,publishing a book,or appearing on podcasts or TV shows.\\n7. Monetize your expertise: Develop a monetization strategy that leverages your expertise and products to generate revenue.\\n8. Build a team: As your business grows,build a team of experts to help you scale your business and provide excellent service to your clients.\\n9. Continual improvement: Continually improve your products,services,and marketing strategies to stay ahead of the competition and provide the best possible experience for your clients.\\n10. Focus on impact: Ultimately,focus on making a positive impact in the lives of your clients and using your expertise to make a difference in the world.\\n[|Human|] What are the mental triggers in Jeff Walker\\'s Product Launch Formula and \"Launch\" book?\\n[|AI|] Here are the mental triggers described in Jeff Walker\\'s Product Launch Formula and \"Launch\" book:\\n\\n1. Scarcity: Creating a sense of urgency by limiting the availability of the product or service being launched.\\n2. Authority: Establishing oneself as an expert or authority in the industry to build credibility and trust with the audience.\\n3. Social proof: Using testimonials,case studies,and other forms of social proof to demonstrate the effectiveness and value of the product or service.\\n4. Reciprocity: Offering something of value to the audience before asking for a sale or commitment.\\n5. Likeability: Building a personal connection with the audience and creating a likable persona to increase the chances of a sale.\\n6. Anticipation: Building anticipation and excitement leading up to the launch to generate buzz and increase interest in the product or service.\\n7. Exclusivity: Creating a feeling of exclusivity by offering early access or special deals to a select group of people.\\n8. Belonging: Creating a sense of community or belonging among the audience to increase engagement and loyalty to the brand.\\n9. Storytelling: Using storytelling to connect with the audience on an emotional level and make the product or service more relatable and memorable.\\n\\nBy leveraging these mental triggers,businesses can create an effective product launch strategy that resonates with their target audience and drives sales and growth.\\n[|Human|] Write a summary of why scarcity and urgency are the strongest mental triggers and have been the driving force behind many of our best performing campaigns over the last 8 years.\\n[|AI|] Scarcity and urgency are two of the strongest mental triggers in marketing because they create a sense of exclusivity and importance around a product or service. By creating a sense of limited availability or a deadline for a promotion,businesses can motivate customers to take action and make a purchase before it\\'s too late.\\n\\nOver the last eight years,many businesses have leveraged these triggers in their marketing campaigns to great success. Scarcity and urgency have been used to sell everything from digital products to physical goods and services,and have proven to be highly effective in driving conversions and increasing revenue.\\n\\nThis is because scarcity and urgency tap into the natural human desire for instant gratification and fear of missing out (FOMO). When customers believe that they may miss out on a valuable opportunity if they don\\'t act quickly,they are more likely to take action and make a purchase.\\n\\nFurthermore,scarcity and urgency can be combined with other mental triggers,such as social proof and authority,to create even more powerful marketing campaigns. By demonstrating the popularity and effectiveness of a product or service,businesses can increase the perceived value and urgency of a promotion,leading to even more sales and revenue.\\n\\nOverall,scarcity and urgency are powerful mental triggers that have been proven to drive results in marketing campaigns. By leveraging these triggers,businesses can create a sense of urgency and exclusivity around their products or services,motivating customers to take action and make a purchase before it\\'s too late.\\n[|Human|] Summarize Russell Brunson\\'s Perfect Webinar Script...\\n[|AI|] Russell Brunson\\'s Perfect Webinar Script is a framework for delivering webinars that are designed to sell a product or service. Here are the key components of the Perfect Webinar Script:\\n\\n1. Introduction: The webinar starts with an introduction that establishes the presenter\\'s credibility and establishes the topic.\\n2. Story: The presenter then tells a personal story that is related to the topic and establishes a connection with the audience.\\n3. Content: The presenter provides valuable content related to the topic,demonstrating their expertise and building trust with the audience.\\n4. Offer: The presenter then introduces the product or service they are selling,explaining how it solves the audience\\'s problem and providing social proof to support their claims.\\n5. Bonuses: The presenter offers bonuses to sweeten the deal and make the offer more enticing.\\n6. Scarcity: The presenter creates a sense of urgency by limiting the availability of the offer or including a deadline for purchase.\\n7. FAQs: The presenter answers common questions about the offer,addressing objections and building trust with the audience.\\n8. Closing: The presenter then delivers a clear call to action,directing the audience to take a specific action to purchase the product or service.\\n\\nOverall,the Perfect Webinar Script is designed to build trust with the audience,establish the presenter\\'s credibility,and provide valuable content,while also creating a sense of urgency and scarcity around the offer being presented. By following this framework,businesses can use webinars as a powerful tool to sell their products and services to a targeted audience.\\n[|Human|] Summarize the 6 human needs as Tony Robbins explains...\\n[|AI|] Tony Robbins describes six core human needs that drive our behaviors and motivations. These six needs are:\\n\\n1. Certainty: The need for safety,stability,and predictability. This includes the need for comfort,security,and control over our environment.\\n2. Variety: The need for novelty,excitement,and change. This includes the need for adventure,stimulation,and new experiences.\\n3. Significance: The need to feel important,special,and unique. This includes the need for recognition,achievement,and respect.\\n4. Connection: The need for social connection and love. This includes the need for intimacy,belonging,and friendship.\\n5. Growth: The need for personal growth and development. This includes the need for learning,self-improvement,and progress.\\n6. Contribution: The need to make a positive impact in the world. This includes the need to give back,help others,and make a difference.\\n\\nAccording to Tony Robbins,these six needs are universal and apply to all individuals. Each person may prioritize these needs differently,and may fulfill them in different ways,but they are fundamental drivers of human behavior. By understanding these needs,individuals can gain insight into their own motivations and behaviors,and can use this knowledge to create a more fulfilling and meaningful life.\\n\\n[|Con người|]'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "share[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22dd05fc110b4ac5998ffa1e03aee374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/39 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "295274695"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "share.to_json('../../data/training_english/share_gpt_38k.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quora = Dataset.from_json('../../data/translated/quora_chat_data_translated.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quora.shuffle()[0]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quora.shuffle().select(range(1000)).map(lambda x: {'len':len(tokenizer.tokenize(x['prompt']))}).to_pandas().hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_quora = quora.shuffle().select(range(20000))\n",
    "temp_quora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Cuộc trò chuyện giữa con người và trợ lý AI.\\n'\n",
    "def mapper(x):\n",
    "    return {\n",
    "        'input': text,\n",
    "        'output': x['prompt'].removeprefix(text) + '\\n[|Con người|]',\n",
    "    }\n",
    "\n",
    "temp_quora = temp_quora.map(mapper, batched=False, remove_columns=['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_quora.to_json('../../data/training_3_8/quora_20k_samples.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OASST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /Users/phamhoang1408/.cache/huggingface/datasets/json/default-c507d95be8d43b3a/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2fdc6c541f64fa6b1f5d53d24c5a6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa469962254546bf9660abf11f235d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc6093371c14a789d7cd100aa004ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/phamhoang1408/.cache/huggingface/datasets/json/default-c507d95be8d43b3a/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# en_oasst = Dataset.from_json('../../data/translated/en_oasst_translated.json')\n",
    "# en_oasst = en_oasst.rename_column('text_translated','text')\n",
    "# vi_oasst = Dataset.from_json('../../data/original/oasst/vi_oasst.json')\n",
    "\n",
    "ds = Dataset.from_json('/Users/phamhoang1408/Desktop/Phase 2 Viettel/main_repo/data/original/oasst/en_oasst.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The conversation between human and AI assistant.\\n[|Human|] Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\\n[|AI|] \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.\\n[|Human|] Now explain it to a dog\\n'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oasst = concatenate_datasets([en_oasst,vi_oasst],axis=0)\n",
    "oasst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Cuộc trò chuyện giữa con người và trợ lý AI.\\n'\n",
    "text = 'The conversation between human and AI assistant.\\n'\n",
    "def mapper(x):\n",
    "    return {\n",
    "        'input': text,\n",
    "        'output': x['text'].removeprefix(text) + '\\n[|Con người|]',\n",
    "    }\n",
    "\n",
    "temp_oasst = oasst.map(mapper, batched=False, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_oasst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_oasst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_oasst.to_json('../../data/training_3_8/oasst_21k_samples.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oasst.shuffle().select(range(1000)).map(lambda x: {'len':len(tokenizer.tokenize(x['text']))}).to_pandas().hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_oasst.shuffle()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_oasst.to_json('../../data/training_3_8/oasst_20k_samples.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALPACA CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca = Dataset.from_json('../../data/training_31_7/alpaca_chat_15k.jsonl')\n",
    "alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Cuộc trò chuyện giữa con người và trợ lý AI.\\n'\n",
    "def mapper(x):\n",
    "    return {\n",
    "        'input': text,\n",
    "        'output': x['input'].removeprefix(text)\n",
    "    }\n",
    "temp_alpaca = alpaca.map(mapper, batched=False, remove_columns=['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_alpaca[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_alpaca.to_json('../../data/training_3_8/alpaca_chat_15k_samples.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT4 INSTRUCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4 = Dataset.from_json('../../data/translated/gpt4_instruct_similarity_0_9_translated.json')\n",
    "gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4['instruction_translated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = \"Cuộc trò chuyện giữa con người và trợ lý AI.\\n[|Con người|] {instruction_translated}\\n{input_translated}\\n[|AI|] \"\n",
    "template2 = \"Cuộc trò chuyện giữa con người và trợ lý AI.\\n[|Con người|] {instruction_translated}\\n[|AI|] \"\n",
    "\n",
    "def mapper(x):\n",
    "    if x['input_translated'] != '' and x['input_translated'] is not None:\n",
    "        template = template1.format(\n",
    "            instruction_translated=x['instruction_translated'], \n",
    "            input_translated=x['input_translated']\n",
    "        )\n",
    "    else:\n",
    "        template = template2.format(instruction_translated=x['instruction_translated'])\n",
    "    return {\n",
    "        'input': template,\n",
    "        'output': x['response_translated'] + '\\n[|Con người|]'\n",
    "    }\n",
    "\n",
    "temp_gpt4 = gpt4.map(mapper,batched=False, remove_columns=gpt4.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = temp_gpt4.shuffle()[0]\n",
    "print(a['input']+a['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_gpt4.to_json('../../data/training_3_8/gpt4_instruct.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOLLY WITH CONTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly = Dataset.from_json('../../data/translated/dolly_with_context_translated.jsonl')\n",
    "dolly = dolly.remove_columns(['instruction', 'context', 'response', 'category_translated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly.shuffle()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = \"Cuộc trò chuyện giữa con người và trợ lý AI.\\n[|Con người|] Trả lời câu hỏi dựa vào đoạn văn bản dưới đây. Chỉ được trả lời dựa trên thông tin nằm trong văn bản được cung cấp.\\nCâu hỏi: {instruction_translated}\\nĐoạn văn bản:\\n{context_translated}\\n[|AI|] \"\n",
    "\n",
    "def mapper(x):\n",
    "    ip = template1.format(\n",
    "        instruction_translated=x['instruction_translated'], \n",
    "        context_translated=x['context_translated']\n",
    "    )\n",
    "    return {\n",
    "        'input': ip,\n",
    "        'output': x['response_translated'] + '\\n[|Con người|]'\n",
    "    }\n",
    "\n",
    "temp_dolly = dolly.map(mapper,batched=False, remove_columns=dolly.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp_dolly.shuffle()[0]\n",
    "print(temp['input'] + temp['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuộc trò chuyện giữa con người và trợ lý AI.\n",
    "[|Con người|] Trả lời câu hỏi pháp luật dựa vào những điều luật liên quan dưới đây. Chỉ được trả lời dựa trên thông tin nằm trong điều luật được cung cấp.\n",
    "Câu hỏi: Phạm tội trong trạng thái tinh thần bị kích động mạnh bị xử lý thế nào?\n",
    "Điều luật liên quan:\n",
    "Điều 125. Tội giết người trong trạng thái tinh thần bị kích động mạnh 1. Người nào giết người trong trạng thái tinh thần bị kích động mạnh do hành vi trái pháp luật nghiêm trọng của nạn nhân đối với người đó hoặc đối với người thân thích của người đó, thì bị phạt tù từ 06 tháng đến 03 năm. 2. Phạm tội đối với 02 người trở lên, thì bị phạt tù từ 03 năm đến 07 năm.\n",
    "[|AI|] Giết người trong trạng thái tinh thần bị kích động mạnh Tội giết người trong trạng thái tinh thần bị kích động mạnh được quy định tại Điều 125 Bộ luật Hình sự 2015 sửa đổi, bổ sung 2017 với mức hình phạt như sau: Khung 1: Người nào giết người trong trạng thái tinh thần bị kích động mạnh do hành vi trái pháp luật nghiêm trọng của nạn nhân đối với người đó hoặc đối với người thân thích của người đó, thì bị phạt tù từ 06 tháng đến 03 năm. Khung 2: Phạm tội đối với 02 người trở lên, thì bị phạt tù từ 03 năm đến 07 năm.\n",
    "[|Con người|]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dolly.to_json('../../data/training_3_8/dolly_4k_samples.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "def load_dataset(folder_path, dataset_size=None):\n",
    "    data = []\n",
    "    for path in glob(folder_path + \"/*.jsonl\"):\n",
    "        ds = Dataset.from_json(path)\n",
    "        for check in ['input', 'output']:\n",
    "            if check not in ds.column_names:\n",
    "                raise ValueError(\"Dataset must have input, output columns\")\n",
    "        # take only the input and output columns\n",
    "        ds = ds.select_columns(['input', 'output'])\n",
    "        data.append(ds)\n",
    "    if dataset_size is None:\n",
    "        return concatenate_datasets(data, axis=0).shuffle(seed=42)\n",
    "    return (\n",
    "        concatenate_datasets(data, axis=0).shuffle(seed=42).select(range(dataset_size))\n",
    "    )\n",
    "\n",
    "def clean(ds):\n",
    "    def mapper(x):\n",
    "        for column in ds.column_names:\n",
    "            x[column] = x[column].replace('[|Human|]','[|Con người|]').replace('\\n[|Con người|]\\n[|Con người|]', '\\n[|Con người|]')\n",
    "        return x\n",
    "    def filter(x):\n",
    "        for column in ds.column_names:\n",
    "            if x[column] == '' or x[column] is None:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    new_ds = ds.map(mapper,batched=False)\n",
    "    new_ds = new_ds.filter(filter)\n",
    "    return new_ds        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('../../data/training_3_8')\n",
    "ds = clean(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ds.shuffle()[0]\n",
    "print(a['input'] + a['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = '../../data/temp_training/all_7_8.jsonl'\n",
    "ds.shuffle().select(range(60000)).to_json(path_to_save, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.shuffle().select(range(1000)).map(lambda x: {'len':len(tokenizer.tokenize(x['input']+x['output']))}).to_pandas().hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_length = df.apply(lambda x: len(tokenizer.tokenize(x['input']+x['output'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df[df_length <= 512].reset_index(drop=True)\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = temp_df.sample(60000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_json('../../data/temp_training/final_dataset.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_json('/Users/phamhoang1408/Desktop/Phase 2 Viettel/main_repo/data/original/formatted_toolformer-similarity-0.8-dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.shuffle()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
