{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "sns.set()\n",
    "rcParams['figure.figsize'] = (20,10)\n",
    "pd.options.display.max_columns = None\n",
    "warnings.filterwarnings('ignore')\n",
    "from pprint import pprint\n",
    "import json\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Official evaluation script for SQuAD version 2.0.\n",
    "\n",
    "In addition to basic functionality, we also compute additional statistics and\n",
    "plot precision-recall curves if an additional na_prob.json file is provided.\n",
    "This file is expected to map question ID's to the model's predicted probability\n",
    "that a question is unanswerable.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "\n",
    "OPTS = None\n",
    "\n",
    "def parse_args():\n",
    "  parser = argparse.ArgumentParser('Official evaluation script for SQuAD version 2.0.')\n",
    "  parser.add_argument('data_file', metavar='data.json', help='Input data JSON file.')\n",
    "  parser.add_argument('pred_file', metavar='pred.json', help='Model predictions.')\n",
    "  parser.add_argument('--out-file', '-o', metavar='eval.json',\n",
    "                      help='Write accuracy metrics to file (default is stdout).')\n",
    "  parser.add_argument('--na-prob-file', '-n', metavar='na_prob.json',\n",
    "                      help='Model estimates of probability of no answer.')\n",
    "  parser.add_argument('--na-prob-thresh', '-t', type=float, default=1.0,\n",
    "                      help='Predict \"\" if no-answer probability exceeds this (default = 1.0).')\n",
    "  parser.add_argument('--out-image-dir', '-p', metavar='out_images', default=None,\n",
    "                      help='Save precision-recall curves to directory.')\n",
    "  parser.add_argument('--verbose', '-v', action='store_true')\n",
    "  if len(sys.argv) == 1:\n",
    "    parser.print_help()\n",
    "    sys.exit(1)\n",
    "  return parser.parse_args()\n",
    "\n",
    "def make_qid_to_has_ans(dataset):\n",
    "  qid_to_has_ans = {}\n",
    "  for article in dataset:\n",
    "    for p in article['paragraphs']:\n",
    "      for qa in p['qas']:\n",
    "        qid_to_has_ans[qa['id']] = bool(qa['answers'])\n",
    "  return qid_to_has_ans\n",
    "\n",
    "def normalize_answer(s):\n",
    "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "  # def remove_articles(text):\n",
    "  #   regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "  #   return re.sub(regex, ' ', text)\n",
    "  def white_space_fix(text):\n",
    "    return ' '.join(text.split())\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in text if ch not in exclude)\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "  return white_space_fix(remove_punc(lower(s)))\n",
    "\n",
    "def get_tokens(s):\n",
    "  if not s: return []\n",
    "  return normalize_answer(s).split()\n",
    "\n",
    "def compute_exact(a_gold, a_pred):\n",
    "  return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "def compute_pre_rec_f1(a_gold, a_pred):\n",
    "  gold_toks = get_tokens(a_gold)\n",
    "  pred_toks = get_tokens(a_pred)\n",
    "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "  num_same = sum(common.values())\n",
    "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "    return int(gold_toks == pred_toks)\n",
    "  if num_same == 0:\n",
    "    return 0, 0, 0\n",
    "  precision = 1.0 * num_same / len(pred_toks)\n",
    "  recall = 1.0 * num_same / len(gold_toks)\n",
    "  f1 = (2 * precision * recall) / (precision + recall)\n",
    "  return precision, recall, f1\n",
    "\n",
    "def get_raw_scores(dataset, preds):\n",
    "  exact_scores = {}\n",
    "  f1_scores = {}\n",
    "  for article in dataset:\n",
    "    for p in article['paragraphs']:\n",
    "      for qa in p['qas']:\n",
    "        qid = qa['id']\n",
    "        gold_answers = [a['text'] for a in qa['answers']\n",
    "                        if normalize_answer(a['text'])]\n",
    "        if not gold_answers:\n",
    "          # For unanswerable questions, only correct answer is empty string\n",
    "          gold_answers = ['']\n",
    "        if qid not in preds:\n",
    "          print('Missing prediction for %s' % qid)\n",
    "          continue\n",
    "        a_pred = preds[qid]\n",
    "        # Take max over all gold answers\n",
    "        exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n",
    "        f1_scores[qid] = max(compute_pre_rec_f1(a, a_pred) for a in gold_answers)\n",
    "  return exact_scores, f1_scores\n",
    "\n",
    "def apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n",
    "  new_scores = {}\n",
    "  for qid, s in scores.items():\n",
    "    pred_na = na_probs[qid] > na_prob_thresh\n",
    "    if pred_na:\n",
    "      new_scores[qid] = float(not qid_to_has_ans[qid])\n",
    "    else:\n",
    "      new_scores[qid] = s\n",
    "  return new_scores\n",
    "\n",
    "def make_eval_dict(exact_scores, f1_scores, qid_list=None):\n",
    "  if not qid_list:\n",
    "    total = len(exact_scores)\n",
    "    return collections.OrderedDict([\n",
    "        ('exact', 100.0 * sum(exact_scores.values()) / total),\n",
    "        ('f1', 100.0 * sum(f1_scores.values()) / total),\n",
    "        ('total', total),\n",
    "    ])\n",
    "  else:\n",
    "    total = len(qid_list)\n",
    "    return collections.OrderedDict([\n",
    "        ('exact', 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n",
    "        ('f1', 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n",
    "        ('total', total),\n",
    "    ])\n",
    "\n",
    "def merge_eval(main_eval, new_eval, prefix):\n",
    "  for k in new_eval:\n",
    "    main_eval['%s_%s' % (prefix, k)] = new_eval[k]\n",
    "\n",
    "def plot_pr_curve(precisions, recalls, out_image, title):\n",
    "  plt.step(recalls, precisions, color='b', alpha=0.2, where='post')\n",
    "  plt.fill_between(recalls, precisions, step='post', alpha=0.2, color='b')\n",
    "  plt.xlabel('Recall')\n",
    "  plt.ylabel('Precision')\n",
    "  plt.xlim([0.0, 1.05])\n",
    "  plt.ylim([0.0, 1.05])\n",
    "  plt.title(title)\n",
    "  plt.savefig(out_image)\n",
    "  plt.clf()\n",
    "\n",
    "def make_precision_recall_eval(scores, na_probs, num_true_pos, qid_to_has_ans,\n",
    "                               out_image=None, title=None):\n",
    "  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
    "  true_pos = 0.0\n",
    "  cur_p = 1.0\n",
    "  cur_r = 0.0\n",
    "  precisions = [1.0]\n",
    "  recalls = [0.0]\n",
    "  avg_prec = 0.0\n",
    "  for i, qid in enumerate(qid_list):\n",
    "    if qid_to_has_ans[qid]:\n",
    "      true_pos += scores[qid]\n",
    "    cur_p = true_pos / float(i+1)\n",
    "    cur_r = true_pos / float(num_true_pos)\n",
    "    if i == len(qid_list) - 1 or na_probs[qid] != na_probs[qid_list[i+1]]:\n",
    "      # i.e., if we can put a threshold after this point\n",
    "      avg_prec += cur_p * (cur_r - recalls[-1])\n",
    "      precisions.append(cur_p)\n",
    "      recalls.append(cur_r)\n",
    "  if out_image:\n",
    "    plot_pr_curve(precisions, recalls, out_image, title)\n",
    "  return {'ap': 100.0 * avg_prec}\n",
    "\n",
    "def run_precision_recall_analysis(main_eval, exact_raw, f1_raw, na_probs, \n",
    "                                  qid_to_has_ans, out_image_dir):\n",
    "  if out_image_dir and not os.path.exists(out_image_dir):\n",
    "    os.makedirs(out_image_dir)\n",
    "  num_true_pos = sum(1 for v in qid_to_has_ans.values() if v)\n",
    "  if num_true_pos == 0:\n",
    "    return\n",
    "  pr_exact = make_precision_recall_eval(\n",
    "      exact_raw, na_probs, num_true_pos, qid_to_has_ans,\n",
    "      out_image=os.path.join(out_image_dir, 'pr_exact.png'),\n",
    "      title='Precision-Recall curve for Exact Match score')\n",
    "  pr_f1 = make_precision_recall_eval(\n",
    "      f1_raw, na_probs, num_true_pos, qid_to_has_ans,\n",
    "      out_image=os.path.join(out_image_dir, 'pr_f1.png'),\n",
    "      title='Precision-Recall curve for F1 score')\n",
    "  oracle_scores = {k: float(v) for k, v in qid_to_has_ans.items()}\n",
    "  pr_oracle = make_precision_recall_eval(\n",
    "      oracle_scores, na_probs, num_true_pos, qid_to_has_ans,\n",
    "      out_image=os.path.join(out_image_dir, 'pr_oracle.png'),\n",
    "      title='Oracle Precision-Recall curve (binary task of HasAns vs. NoAns)')\n",
    "  merge_eval(main_eval, pr_exact, 'pr_exact')\n",
    "  merge_eval(main_eval, pr_f1, 'pr_f1')\n",
    "  merge_eval(main_eval, pr_oracle, 'pr_oracle')\n",
    "\n",
    "def histogram_na_prob(na_probs, qid_list, image_dir, name):\n",
    "  if not qid_list:\n",
    "    return\n",
    "  x = [na_probs[k] for k in qid_list]\n",
    "  weights = np.ones_like(x) / float(len(x))\n",
    "  plt.hist(x, weights=weights, bins=20, range=(0.0, 1.0))\n",
    "  plt.xlabel('Model probability of no-answer')\n",
    "  plt.ylabel('Proportion of dataset')\n",
    "  plt.title('Histogram of no-answer probability: %s' % name)\n",
    "  plt.savefig(os.path.join(image_dir, 'na_prob_hist_%s.png' % name))\n",
    "  plt.clf()\n",
    "\n",
    "def find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n",
    "  num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n",
    "  cur_score = num_no_ans\n",
    "  best_score = cur_score\n",
    "  best_thresh = 0.0\n",
    "  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
    "  for i, qid in enumerate(qid_list):\n",
    "    if qid not in scores: continue\n",
    "    if qid_to_has_ans[qid]:\n",
    "      diff = scores[qid]\n",
    "    else:\n",
    "      if preds[qid]:\n",
    "        diff = -1\n",
    "      else:\n",
    "        diff = 0\n",
    "    cur_score += diff\n",
    "    if cur_score > best_score:\n",
    "      best_score = cur_score\n",
    "      best_thresh = na_probs[qid]\n",
    "  return 100.0 * best_score / len(scores), best_thresh\n",
    "\n",
    "def find_best_thresh_v2(preds, scores, na_probs, qid_to_has_ans):\n",
    "  num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n",
    "  cur_score = num_no_ans\n",
    "  best_score = cur_score\n",
    "  best_thresh = 0.0\n",
    "  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
    "  for i, qid in enumerate(qid_list):\n",
    "    if qid not in scores: continue\n",
    "    if qid_to_has_ans[qid]:\n",
    "      diff = scores[qid]\n",
    "    else:\n",
    "      if preds[qid]:\n",
    "        diff = -1\n",
    "      else:\n",
    "        diff = 0\n",
    "    cur_score += diff\n",
    "    if cur_score > best_score:\n",
    "      best_score = cur_score\n",
    "      best_thresh = na_probs[qid]\n",
    "\n",
    "  has_ans_score, has_ans_cnt = 0, 0\n",
    "  for qid in qid_list:\n",
    "    if not qid_to_has_ans[qid]: continue\n",
    "    has_ans_cnt += 1\n",
    "\n",
    "    if qid not in scores: continue\n",
    "    has_ans_score += scores[qid]\n",
    "\n",
    "  return 100.0 * best_score / len(scores), best_thresh, 1.0 * has_ans_score / has_ans_cnt\n",
    "\n",
    "def find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n",
    "  best_exact, exact_thresh = find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)\n",
    "  best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n",
    "  main_eval['best_exact'] = best_exact\n",
    "  main_eval['best_exact_thresh'] = exact_thresh\n",
    "  main_eval['best_f1'] = best_f1\n",
    "  main_eval['best_f1_thresh'] = f1_thresh\n",
    "\n",
    "def find_all_best_thresh_v2(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n",
    "  best_exact, exact_thresh, has_ans_exact = find_best_thresh_v2(preds, exact_raw, na_probs, qid_to_has_ans)\n",
    "  best_f1, f1_thresh, has_ans_f1 = find_best_thresh_v2(preds, f1_raw, na_probs, qid_to_has_ans)\n",
    "  main_eval['best_exact'] = best_exact\n",
    "  main_eval['best_exact_thresh'] = exact_thresh\n",
    "  main_eval['best_f1'] = best_f1\n",
    "  main_eval['best_f1_thresh'] = f1_thresh\n",
    "  main_eval['has_ans_exact'] = has_ans_exact\n",
    "  main_eval['has_ans_f1'] = has_ans_f1\n",
    "\n",
    "def main():\n",
    "  with open(OPTS.data_file) as f:\n",
    "    dataset_json = json.load(f)\n",
    "    dataset = dataset_json['data']\n",
    "  with open(OPTS.pred_file) as f:\n",
    "    preds = json.load(f)\n",
    "\n",
    "  new_orig_data = []\n",
    "  for article in dataset:\n",
    "    for p in article['paragraphs']:\n",
    "      for qa in p['qas']:\n",
    "        if qa['id'] in preds:\n",
    "          new_para = {'qas': [qa]}\n",
    "          new_article = {'paragraphs': [new_para]}\n",
    "          new_orig_data.append(new_article)\n",
    "  dataset = new_orig_data\n",
    "\n",
    "  if OPTS.na_prob_file:\n",
    "    with open(OPTS.na_prob_file) as f:\n",
    "      na_probs = json.load(f)\n",
    "  else:\n",
    "    na_probs = {k: 0.0 for k in preds}\n",
    "  qid_to_has_ans = make_qid_to_has_ans(dataset)  # maps qid to True/False\n",
    "  has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
    "  no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
    "  exact_raw, f1_raw = get_raw_scores(dataset, preds)\n",
    "  exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans,\n",
    "                                        OPTS.na_prob_thresh)\n",
    "  f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans,\n",
    "                                     OPTS.na_prob_thresh)\n",
    "  out_eval = make_eval_dict(exact_thresh, f1_thresh)\n",
    "  if has_ans_qids:\n",
    "    has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n",
    "    merge_eval(out_eval, has_ans_eval, 'HasAns')\n",
    "  if no_ans_qids:\n",
    "    no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n",
    "    merge_eval(out_eval, no_ans_eval, 'NoAns')\n",
    "  if OPTS.na_prob_file:\n",
    "    find_all_best_thresh(out_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans)\n",
    "  if OPTS.na_prob_file and OPTS.out_image_dir:\n",
    "    run_precision_recall_analysis(out_eval, exact_raw, f1_raw, na_probs, \n",
    "                                  qid_to_has_ans, OPTS.out_image_dir)\n",
    "    histogram_na_prob(na_probs, has_ans_qids, OPTS.out_image_dir, 'hasAns')\n",
    "    histogram_na_prob(na_probs, no_ans_qids, OPTS.out_image_dir, 'noAns')\n",
    "  if OPTS.out_file:\n",
    "    with open(OPTS.out_file, 'w') as f:\n",
    "      json.dump(out_eval, f)\n",
    "  else:\n",
    "    print(json.dumps(out_eval, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /Users/phamhoang1408/.cache/huggingface/datasets/json/default-c289026bcd51adc6/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c4ece2887b4ff1857b51ec1b863622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97a90e9d63d4ed7a48756d3fa17ff46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ccbd2de1594964ac921c7341af04f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/phamhoang1408/.cache/huggingface/datasets/json/default-c289026bcd51adc6/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5bd4ad0000944d49093a779d379bc67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = Dataset.from_json('./check_data/temp.jsonl')\n",
    "ds = ds.filter(lambda x: x['pred'] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for p in ds:\n",
    "    f1, pre, rec = [], [], []\n",
    "    for a in p['answer']:\n",
    "        temp = compute_pre_rec_f1(a, p['pred'])\n",
    "        pre.append(temp[0])\n",
    "        rec.append(temp[1])\n",
    "        f1.append(temp[2])\n",
    "    p['f1'] = max(f1)\n",
    "    p['pre'] = max(pre)\n",
    "    p['rec'] = max(rec)\n",
    "    results.append(p)\n",
    "\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>pre</th>\n",
       "      <th>rec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2579.000000</td>\n",
       "      <td>2579.000000</td>\n",
       "      <td>2579.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.276767</td>\n",
       "      <td>0.225706</td>\n",
       "      <td>0.584141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.256537</td>\n",
       "      <td>0.250432</td>\n",
       "      <td>0.398172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.047415</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.414104</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                f1          pre          rec\n",
       "count  2579.000000  2579.000000  2579.000000\n",
       "mean      0.276767     0.225706     0.584141\n",
       "std       0.256537     0.250432     0.398172\n",
       "min       0.000000     0.000000     0.000000\n",
       "25%       0.074074     0.047415     0.192308\n",
       "50%       0.210526     0.142857     0.650000\n",
       "75%       0.414104     0.312500     1.000000\n",
       "max       1.000000     1.000000     1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['f1', 'pre', 'rec']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Một hội nghị hòa bình đã được Liên quân tổ chức trên vùng lãnh thổ Iraq bị chiếm đóng. Tại hội nghị, Iraq được phép sử dụng các máy bay trực thăng vũ trang trong phía biên giới lãnh thổ nước mình, bề ngoài là để vận chuyển các quan chức chính phủ do những thiệt hại đã phải hứng chịu của hệ thống vận chuyển công cộng. Một thời gian ngắn sau, những chiếc trực thăng đó - và đa số các lực lượng vũ trang Iraq - lại quay sang phục vụ mục tiêu chiến đấu chống lại cuộc nổi dậy của người Shiite ở phía nam. Tại phía bắc, các lãnh đạo người Kurd tin tưởng vào những lời tuyên bố của Hoa Kỳ rằng họ sẽ ủng hộ một cuộc nổi dậy và bắt đầu chiến đấu, với hy vọng thực hiện được một cuộc đảo chính. Tuy nhiên, khi những giúp đỡ của Mỹ còn chưa tới nơi, các tướng lĩnh Iraq còn trung thành đã đàn áp dã man quân đội người Kurd. Hàng triệu người Kurd đã phải chạy qua các vùng núi để đến những vùng người Kurd tại Thổ Nhĩ Kỳ và Iran. Những vụ xung đột đó sau này đã dẫn tới việc thành lập những vùng cấm bay ở cả phía bắc và phía nam Iraq. Tại Kuwait, gia đình Emir được tái lập và những kẻ bị cho là cộng tác với Iraq bị đàn áp. Cuối cùng, hơn 400.000 người đã bị trục xuất khỏi đất nước, gồm một số lượng lớn người Palestine (vì sự ủng hộ và hợp tác của họ với Saddam Hussein).',\n",
       " 'question': 'Phe liên minh đã tổ chức hội nghị hoà bình tại nơi nào?',\n",
       " 'answer': ['vùng lãnh thổ Iraq bị chiếm đóng',\n",
       "  'vùng lãnh thổ Iraq bị chiếm đóng',\n",
       "  'vùng lãnh thổ Iraq bị chiếm đóng',\n",
       "  'vùng lãnh thổ Iraq bị chiếm đóng'],\n",
       " 'pred': 'Hội nghị hòa bình đã được Liên quân tổ chức trên vùng lãnh thổ Iraq bị chiếm đóng. Tại hội nghị, Iraq được phép sử dụng các máy bay trực thăng vũ trang trong phía biên giới lãnh thổ nước mình, bề ngoài là để vận chuyển các quan chức chính phủ do những thiệt hại đã phải hứng chịu của hệ thống vận chuyển công cộng. Một thời gian ngắn sau, những chiếc trực thăng đó - và đa số các lực lượng vũ trang Iraq - lại quay sang phục vụ mục tiêu chiến đấu chống lại cuộc nổi dậy của người Shiite ở phía nam. Tại phía bắc, các lãnh đạo người Kurd tin tưởng vào những lời tuyên bố của Hoa Kỳ rằng họ sẽ ủng hộ một cuộc nổi dậy và bắt đầu chiến đấu, với hy vọng thực hiện được một cuộc đảo chính. Tuy nhiên, khi những giúp đỡ của Mỹ còn chưa tới nơi, các tướng lĩnh Iraq còn trung thành đã đàn áp dã man quân đội người Kurd. Hàng triệu người Kurd đã phải chạy qua các vùng núi để đến những vùng người Kurd tại Thổ Nhĩ Kỳ và Iran. Những vụ xung đột đó sau này đã dẫn tới việc thành lập những vùng cấm bay ở cả phía bắc và phía nam Iraq. Tại Kuwait, gia đình Emir được tái lập và những kẻ bị cho là'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.shuffle()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
