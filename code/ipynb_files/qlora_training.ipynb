{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "CUDA SETUP: Loading binary /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so, 0x0006): tried: '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'prepare_model_for_kbit_training' from 'peft' (/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/peft/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/phamhoang1408/Desktop/Phase 2 Viettel/main_repo/code/ipynb_files/qlora_training.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Phase%202%20Viettel/main_repo/code/ipynb_files/qlora_training.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Phase%202%20Viettel/main_repo/code/ipynb_files/qlora_training.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset, concatenate_datasets\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Phase%202%20Viettel/main_repo/code/ipynb_files/qlora_training.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m \u001b[39mimport\u001b[39;00m LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, PeftConfig\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Phase%202%20Viettel/main_repo/code/ipynb_files/qlora_training.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m login\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Phase%202%20Viettel/main_repo/code/ipynb_files/qlora_training.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwandb\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'prepare_model_for_kbit_training' from 'peft' (/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/peft/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, PeftConfig\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_start_run = datetime.now().strftime(\"%Y%m%d-%H%M\").replace('-', '_')\n",
    "args = {\n",
    "    'hugging_face_api_key': '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',\n",
    "    'wandb_api_key': '59759d7f774b09319a3e0e3aebefc7fcf7ccf4f1',\n",
    "    'dataset_paths': [\n",
    "        '../../data/training/share_gpt_no_code_conversations_40k_translated.json',\n",
    "    ],\n",
    "    'base_model_name': 'bigscience/bloomz-7b',\n",
    "    'tokenizer_args': {\n",
    "        'max_length': 2048,\n",
    "        'padding': 'max_length',\n",
    "        'truncation': True,\n",
    "    },\n",
    "    'lora_args': {\n",
    "        'rank': 16,\n",
    "        'alpha': 32,\n",
    "        'dropout': 0.05,\n",
    "    },\n",
    "    'training_args': {\n",
    "        'output_dir': 'checkpoints_' + date_time_start_run,\n",
    "        'num_train_epochs': 1,\n",
    "        'per_device_train_batch_size': 4,\n",
    "        'gradient_accumulation_steps': 4,\n",
    "        'optim': 'paged_adamw_8bit',\n",
    "        'logging_steps': 100,\n",
    "        'learning_rate': 2e-4,\n",
    "        'fp16': True,\n",
    "        'warmup_ratio': 0.05,\n",
    "        'lr_scheduler_type': 'cosine',\n",
    "        'report_to': 'wandb',\n",
    "        'push_to_hub': True,\n",
    "        'group_by_length': True,\n",
    "        'max_steps': 1,\n",
    "    },\n",
    "    'hub_adapter_repo_name': 'chatbot_qlora_' + date_time_start_run,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hub login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(args['huggingface_api_key'])\n",
    "wandb.login(key=args['wandb_api_key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_paths):\n",
    "    data = []\n",
    "    for path in file_paths:\n",
    "        ds = Dataset.from_json(path)\n",
    "        data.append(ds)\n",
    "    return concatenate_datasets(data, axis=0).shuffle()\n",
    "ds = load_dataset(args['dataset_paths'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    args['base_model_name'],\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    device_map='auto',\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "),\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args['base_model_name'], trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.max_length = args['tokenizer_args']['max_length']\n",
    "\n",
    "# load peft model\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(\n",
    "    model,\n",
    "    LoraConfig(\n",
    "    lora_alpha=args['lora_args']['alpha'],\n",
    "    lora_dropout=args['lora_args']['dropout'],\n",
    "    r=args['lora_args']['rank'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"Write a poem\n",
    "\"\"\"\n",
    "encoding = tokenizer(prompt,return_tensors='pt')\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(**encoding)\n",
    "print(tokenizer.decode(outputs[0],skip_special_tokens=True))\n",
    "print(\"Inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ds = ds.map(lambda x: tokenizer(x['text'], padding=True, truncation=True, max_length=args['tokenizer_args']['max_length']), remove_columns=['text'])\n",
    "training_arguments = TrainingArguments(**args['training_args'])\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=training_ds,\n",
    "    args=training_arguments,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./finals')\n",
    "model.push_to_hub(args['hub_adapter_repo_name'])\n",
    "print('\\n-----------Finish-----------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__ > '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ds:\n",
    "\n",
    "0) quora_chat (10k) (chat instruct) (format chat) (medium)\n",
    "1) share gpt no code (40k) (chat instruct) (format chat) (long)\n",
    "2) alpaca chat cleaned (51k) (instruct) (format chat) (short)\n",
    "3) all_faqs (2k7) ==> duplicate resample to (10k) (format chat) (medium)\n",
    "4) dialog sum (10k) (instruct) (format instruct) (medium)\n",
    "5) cnn dailymail (30k) (instruct) (format instruct) (long)\n",
    "6) gpt4_instruct_0.9 (17k) (instruct) (format instruct) (short)\n",
    "\n",
    "### Training size: 100k\n",
    "0) 10k\n",
    "1) 25k\n",
    "2) 15k\n",
    "3) 10k\n",
    "4) 10k\n",
    "5) 15k\n",
    "6) 15k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
