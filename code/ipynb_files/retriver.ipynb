{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "defaut_search_config = {\n",
    "    'k': 1,\n",
    "}\n",
    "class Retriever:\n",
    "    def __init__(self, text_list, embedder, main_tokenizer, search_config=defaut_search_config):\n",
    "        self.text_list = text_list\n",
    "        self.embedder = embedder\n",
    "        self.main_tokenizer = main_tokenizer\n",
    "        self.search_config = search_config\n",
    "        self.corpus = None\n",
    "        self.db = None\n",
    "        self.retriever = None\n",
    "        if self.corpus is None:\n",
    "            self._build()\n",
    "\n",
    "    def _build_corpus(self, num_token):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=num_token,\n",
    "            length_function=lambda x: len(self.main_tokenizer.tokenize(x))\n",
    "        )\n",
    "        self.corpus = []\n",
    "        for i, text in enumerate(self.text_list):\n",
    "            text_chunks = text_splitter.split_text(text)\n",
    "            text_docs = [Document(text_chunk, {'id': i}) for text_chunk in text_chunks]\n",
    "            self.corpus.extend(text_docs)\n",
    "    \n",
    "    def _build(self):\n",
    "        self._build_corpus()\n",
    "        self.db = FAISS.from_documents(self.corpus, self.embedder)\n",
    "        self.retriever = self.db.as_retriever(search_kwargs=self.search_config)\n",
    "\n",
    "    def save_local(self, path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        text_list_df = pd.DataFrame(self.text_list)\n",
    "        corpus_list = [\n",
    "            {\n",
    "                'page_content': doc.page_content,\n",
    "                'metadata': doc.metadata\n",
    "            } for doc in self.corpus\n",
    "        ]\n",
    "        corpus_df = pd.DataFrame(corpus_list)\n",
    "        text_list_df.to_csv(os.path.join(path, 'text_list.csv'))\n",
    "        corpus_df.to_csv(os.path.join(path, 'corpus.csv'))\n",
    "        self.db.save_local(os.path.join(path, 'db'))\n",
    "        with open(os.path.join(path, 'search_config.json'), 'w') as f:\n",
    "            json.dump(self.search_config, f)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_local(self, path, embedder, main_tokenizer):\n",
    "        # check all paths exist\n",
    "        paths = [\n",
    "            os.path.join(path, 'text_list.csv'),\n",
    "            os.path.join(path, 'corpus.csv'),\n",
    "            os.path.join(path, 'db'),\n",
    "            os.path.join(path, 'search_config.json'),\n",
    "        ]\n",
    "        for path in paths:\n",
    "            if not os.path.exists(path):\n",
    "                raise ValueError(f'Path {path} does not exist')\n",
    "        with open(os.path.join(path, 'search_config.json'), 'r') as f:\n",
    "            search_config = json.load(f)\n",
    "        text_list_df = pd.read_csv(os.path.join(path, 'text_list.csv'))\n",
    "        corpus_df = pd.read_csv(os.path.join(path, 'corpus.csv'))\n",
    "        text_list = text_list_df['0'].tolist()\n",
    "        corpus = [\n",
    "            Document(row['page_content'], eval(row['metadata']))\n",
    "            for _, row in corpus_df.iterrows()\n",
    "        ]\n",
    "        db = FAISS.load_local(os.path.join(path, 'db'), embedder)\n",
    "        retriever =  Retriever(text_list, embedder, main_tokenizer, search_config=search_config)\n",
    "        retriever.corpus = corpus\n",
    "        retriever.db = db\n",
    "        retriever.retriever = db.as_retriever(search_kwargs=retriever.search_config)\n",
    "        return retriever\n",
    "\n",
    "    def search_main_document(self, query):\n",
    "        candidate_doc = self.retriever.get_relevant_documents(query)[0]\n",
    "        id_ = candidate_doc.metadata['id']\n",
    "        candidate_chunks = [doc.page_content for doc in self.corpus if doc.metadata['id'] == id_]\n",
    "        temp_embeddings = self.embedder.embed_documents([doc for doc in candidate_chunks])    \n",
    "        return {\n",
    "            'id': id_,\n",
    "            'chunk_texts': candidate_chunks,\n",
    "            'chunk_embeddings': temp_embeddings\n",
    "        }\n",
    "    \n",
    "    def search_chunks(self, main_doc, query, k=2):\n",
    "        q_embedding = self.embedder.embed_query(query)\n",
    "        chunk_embeddings = main_doc['chunk_embeddings']\n",
    "        chunk_texts = main_doc['chunk_texts']\n",
    "        scores = np.dot(chunk_embeddings, q_embedding) / (np.linalg.norm(chunk_embeddings, axis=1) * np.linalg.norm(q_embedding))\n",
    "        top_k = np.argsort(scores)[::-1][:k]\n",
    "        return [chunk_texts[i] for i in top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('hello_dir', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
