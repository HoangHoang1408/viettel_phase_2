{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# prompt templates\n",
    "class PromptType:\n",
    "    TEXT_SUMMARIZATION = \"text_summarization\"\n",
    "    CONVERSATION_SUMMARIZATION = \"conversation_summarization\"\n",
    "    QA_WITH_CONTEXT = \"qa_with_context\"\n",
    "    LAW_WITH_CONTEXT = \"law_with_context\"\n",
    "\n",
    "conversation_system_prompt = \"Cuộc trò chuyện giữa con người và trợ lý AI.\\n\"\n",
    "\n",
    "conversation_summarization_prompt = \"Tóm tắt ngắn gọn đoạn hội thoại sau đây:\\n{conversation}\\nĐoạn hội thoại đã được tóm tắt:\\n\"\n",
    "text_summarization_prompt = conversation_system_prompt + \"[|Con người|] Tóm tắt ngắn gọn đoạn văn bản sau đây:\\n{context}\\n[|AI|] \"\n",
    "qa_with_context_prompt = conversation_system_prompt + \"[|Con người|] Trả lời câu hỏi dựa vào đoạn văn bản dưới đây. Chỉ được trả lời dựa trên thông tin nằm trong văn bản được cung cấp.\\nCâu hỏi: {question}\\nĐoạn văn bản: {context}\\n[|AI|] \"\n",
    "law_with_context_prompt = conversation_system_prompt + \"[|Con người|] Trả lời câu hỏi pháp luật dựa vào những điều luật liên quan dưới đây. Chỉ được trả lời dựa trên thông tin nằm trong điều luật được cung cấp.\\nCâu hỏi: {question}\\nĐiều luật liên quan:\\n{context}\\n[|AI|] \"\n",
    "\n",
    "\n",
    "\n",
    "# factory decorator to save prompt\n",
    "import pandas as pd\n",
    "from time import perf_counter\n",
    "\n",
    "class Inferencer:\n",
    "    def __init__(self):\n",
    "        self.prompt_templates = {\n",
    "            \"text_summarization\": text_summarization_prompt,\n",
    "            \"conversation_summarization\": conversation_summarization_prompt,\n",
    "            \"qa_with_context\": qa_with_context_prompt,\n",
    "            \"law_with_context\": law_with_context_prompt\n",
    "        }\n",
    "        \n",
    "    @staticmethod\n",
    "    def log_prompt(model_name, csv_path_to_save='prompt_log.csv'):\n",
    "        \"\"\"\n",
    "        Decorator to save prompt to csv file\n",
    "        Applied function must return a prompt\n",
    "        \"\"\"\n",
    "        def decorator(func):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                start = perf_counter()\n",
    "                prompt = func(*args, **kwargs)\n",
    "                cur_time = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                total_time = perf_counter() - start\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_path_to_save)\n",
    "                    if set(df.columns.tolist()) != set(['model', 'generated', 'time_taken', 'prompt_time']):\n",
    "                        raise Exception('Columns are not correct')\n",
    "                except:\n",
    "                    os.makedirs(os.path.dirname(csv_path_to_save), exist_ok=True)\n",
    "                    df = pd.DataFrame(columns=['model', 'generated', 'time_taken', 'prompt_time'])\n",
    "                print(f'### Generated in {total_time:.6f} seconds ###\\n')\n",
    "                df = pd.concat([\n",
    "                    df, pd.DataFrame([[model_name, prompt, total_time, cur_time]], \n",
    "                    columns=['model', 'generated', 'time_taken', 'prompt_time'])\n",
    "                ])\n",
    "                df.drop_duplicates(subset=['generated'], keep='last').to_csv(csv_path_to_save, index=False)\n",
    "                return prompt\n",
    "            return wrapper\n",
    "        return decorator\n",
    "    \n",
    "    def format_prompt(self, prompt_type, **kwargs):\n",
    "        if prompt_type not in self.prompt_templates:\n",
    "            raise ValueError(\"Prompt type must be one of the following: {}\".format(self.prompt_templates.keys()))\n",
    "        return self.prompt_templates[prompt_type].format(**kwargs)\n",
    "\n",
    "    @log_prompt(model_name='bloom_sft_3b', csv_path_to_save='../../prompt_logs/prompt_log.csv')\n",
    "    def generate(self, gen_func=lambda x: x, prompt=None):\n",
    "        if prompt is None:\n",
    "            raise ValueError(\"Prompt must not be None\")\n",
    "        return gen_func(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = Inferencer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Generated in 0.000055 seconds ###\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference.generate(prompt=\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hellofromhoang'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
